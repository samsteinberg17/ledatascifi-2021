{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Golden Rules for) Exploratory Data Analysis (EDA)\n",
    "\n",
    "What's below is some hard earned wisdom. Please accept this into your heart and go forth such that data never harms you as it has so many before you. \n",
    "\n",
    "```{figure} https://media.giphy.com/media/Wn74RUT0vjnoU98Hnt/source.gif\n",
    "---\n",
    "height: 300px\n",
    "name: baby-yodSSSSS\n",
    "---\n",
    "You, soon to be wise\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{dropdown} 1. **GOLDEN RULES for EDA**\n",
    "\n",
    "You got new data - fun! But before you go speeding off into analysis, first you must learn some basic facts about the data. \n",
    "\n",
    "You should **always, always, always** open your datasets up to physically (digitally?) look at them[^member] and then also generate summary statistics. Here are _some_ basic outputs you should examine carefully. \n",
    "\n",
    "1. Print the first and last five rows: `df.head()` and `df.tail()`\n",
    "2. What is the shape (# rows and variables) of the data? `df.shape`\n",
    "1. Print the column names\n",
    "3. How much memory does it take, and what are the variable names/types? `df.info()`\n",
    "4. Summary stats on variables: \n",
    "    - `df.describe()` - count, mean, sd, etc\n",
    "    - `df['var'].value_counts()[:10]`  - the most common values of a variable\n",
    "    - `df['var'].nunique()` - the number of unique values of a variable\n",
    "    \n",
    "```{tip}\n",
    "Automate your initial EDA by putting something like the below in your codebook.\n",
    "\n",
    "You can refine this as you go - you'll see more and more tips how to improve your EDA as we go.\n",
    "```\n",
    "\n",
    "```{warning}\n",
    "Two things:\n",
    "1. Don't just run these and move on! Actually **look** at the output and check for possible issues. (Some possible issues are listed in the next drop down.)\n",
    "1. This isn't a comprehensive list of things I'd do to check datasets, merely a reasonable start!\n",
    "\n",
    "```\n",
    "\n",
    "````\n",
    "\n",
    "[^member]: [Remember this?](../01/07_debugging.html#seriously-print-your-data-and-objects-often)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{dropdown} 2. Data cleaning\n",
    "\n",
    "```{admonition} A thought not quite profound enough to be wisdom\n",
    ":class: tip\n",
    "Data cleaning, exploration, and analysis exist in a never ending feedback loop. Analysis projects are rarely linear. You'll be doing some analysis and realize there is a data problem, or that you need new data, and you're back at step one above.\n",
    "```\n",
    " \n",
    "Here are some things you might look for while cleaning your data:\n",
    "\n",
    "| Look for | Comment |\n",
    "| :--- | :--- |\n",
    "Do some variables have large outliers? | You might need to winsorize, or drop, those observations.\n",
    "Do some variables have many missing variables? | Maybe there was a problem with the source or how you loaded it.\n",
    "Do some variables have impossible values? | For example, sales can't be negative! Yet, some datasets use \"-99\" to indicate missing data. Or are there bad dates (June 31st)?\n",
    "Are there duplicate observations? | E.g. two 2001-GM observations because they changed their fiscal year in the middle of calendar year 2001?\n",
    "Are there missing observations or a gap in the time series? | E.g. no observation in 2005 for Enron because the executive team was too \"distracted to file its 10-K?\n",
    "Do variables contradict each other? | E.g. If my birthday is 1/5/1999, then I must be 21 as of this writing.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{dropdown} 3. Institutional knowledge is crucial\n",
    "\n",
    "```{tip}\n",
    "Lehigh offers students free subscriptions to WSJ, NYT, and FT!\n",
    "```\n",
    "\n",
    "- For example, in my research on firm investment policies and financing decisions (e.g. leverage), I drop any firms from the finance or utility industries. I know to do this because those industries are subject to several factors that fundamentally change their leverage in ways unrelated to their investment policies. Thus, including them would contaminate any relationships I find.\n",
    "- When we work with 10-K text data, we need to know what data tables are available and how to remove boilerplate. \n",
    "- You only get institutional through reading supporting materials, documentation, related info (research papers and WSJ, etc), and... the documents themselves. For example, I've read in excess of 6,000 proxy statements for a single research project. (If that sounds exciting: Go to grad school and become a prof!)\n",
    "\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{dropdown} 4. Explore the data with your question in mind\n",
    "\n",
    "- Compute statistics by subgroups (by age, or industry), or two-way (by gender and age)\n",
    "- Do a big correlation matrix to get a sense of possible relationships between variables in the data on a first pass. (We will do this later.)\n",
    "- This step reinforces institutional knowledge and your understanding of the data\n",
    "\n",
    "Remember, **fata exploration** and **data cleaning** are interrelated and you'll go back and forth from one to the other. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy EDA \n",
    "\n",
    "I do all of suggested steps here. The output isn't pretty, and I'd clean it up if I was writing a report. But this gets the ball rolling.\n",
    "\n",
    "```{note}\n",
    "This data only has one categorical variable (species). You should do `value_counts` and `nunique` for all categorical variables.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa \n",
      "---\n",
      "     sepal_length  sepal_width  petal_length  petal_width    species\n",
      "145           6.7          3.0           5.2          2.3  virginica\n",
      "146           6.3          2.5           5.0          1.9  virginica\n",
      "147           6.5          3.0           5.2          2.0  virginica\n",
      "148           6.2          3.4           5.4          2.3  virginica\n",
      "149           5.9          3.0           5.1          1.8  virginica \n",
      "---\n",
      "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
      "       'species'],\n",
      "      dtype='object') \n",
      "---\n",
      "The shape is:  (150, 5) \n",
      "---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "Info: None \n",
      "---\n",
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "count    150.000000   150.000000    150.000000   150.000000\n",
      "mean       5.843333     3.057333      3.758000     1.199333\n",
      "std        0.828066     0.435866      1.765298     0.762238\n",
      "min        4.300000     2.000000      1.000000     0.100000\n",
      "25%        5.100000     2.800000      1.600000     0.300000\n",
      "50%        5.800000     3.000000      4.350000     1.300000\n",
      "75%        6.400000     3.300000      5.100000     1.800000\n",
      "max        7.900000     4.400000      6.900000     2.500000 \n",
      "---\n",
      "versicolor    50\n",
      "setosa        50\n",
      "virginica     50\n",
      "Name: species, dtype: int64 \n",
      "---\n",
      "3 \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# import a famous dataset, seaborn nicely contains it out of the box!\n",
    "import seaborn as sns \n",
    "iris = sns.load_dataset('iris') \n",
    "\n",
    "print(iris.head(),  '\\n---')\n",
    "print(iris.tail(),  '\\n---')\n",
    "print(iris.columns, '\\n---')\n",
    "print(\"The shape is: \",iris.shape, '\\n---')\n",
    "print(\"Info:\",iris.info(), '\\n---') # memory usage, name, dtype, and # of non-null obs (--> # of missing obs) per variable\n",
    "print(iris.describe(), '\\n---') # summary stats, and you can customize the list!\n",
    "print(iris['species'].value_counts()[:10], '\\n---')\n",
    "print(iris['species'].nunique(), '\\n---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
